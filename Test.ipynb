{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**<font size=\"5\">Table of contents</font>** \n",
    "\n",
    " - [**Chapter 0: Introduction**](#Chapter0)\n",
    "     - [**0.1 Statistical methods**](#0.1)\n",
    "     - [**0.2 Data and portfolio construction**](#0.2)\n",
    " - [**Chapter 1: Data and portfolio construction**](#Chapter1)\n",
    "    - [**1.1 High Minus Low (HML) portfolio**](#1.1)\n",
    "    - [**1.2 Winners Minus Losers (WML) portfolio**](#1.2)\n",
    "    - [**1.3 Value Momemtum (HML-WML) Portfolio**](#1.3) \n",
    "    - [**1.4 Market Portfolio (Mkt-RF) portfolio**](#1.4)\n",
    " - [**Chapter 2:  Risk- risk reward ratio's and Data exploration**](#Chapter2)\n",
    "    - [**2.1 Return and risk reward ratio's**](#2.1)\n",
    "    - [**2.2 Exploratory Data Analysis (EDA)**](#2.2)\n",
    " - [**Chapter 3: Simulation analysis Part 1**](#Chapter3)\n",
    "    - [**3.1 Mkt-RF**](#3.1)\n",
    "    - [**3.2 HML**](#3.2)\n",
    "    - [**3.3 WML**](#3.3)\n",
    "    - [**3.4 HML-WML**](#3.4)\n",
    "    - [**3.5 Summarize Results**](#3.5)\n",
    " - [**Chapter 4:  Simulation analysis Part 2**](#Chapter4)\n",
    "    - [**4.1.Simulation Period: 01-01-1963 till 31-12-2003**](#4.1)\n",
    "        - [**4.1.1 Mkt-RF**](#4.1.1)\n",
    "        - [**4.1.2 HML**](#4.1.2)\n",
    "        - [**4.1.3 WML**](#4.1.3)\n",
    "        - [**4.1.4 HML-WML**](#4.1.4)\n",
    "    - [**4.2.Simulation Period: 01-01-1927 till 31-12-2003**](#4.2)\n",
    "        - [**4.2.1 Mkt-RF**](#4.2.1)\n",
    "        - [**4.2.2 HML**](#4.2.2)\n",
    "        - [**4.2.3 WML**](#4.2.3)\n",
    "        - [**4.2.4 HML-WML**](#4.2.4)\n",
    "    - [**4.3 Summarize Results**](#4.3)\n",
    "        - [**4.3.1 Simulation Period: 01-01-1963 till 31-12-2003**](#4.3.1)\n",
    "        - [**4.3.2 Simulation Period: 01-01-1927 till 31-12-2003**](#4.3.2)\n",
    " - [**Chapter 5:  Conclusion and discussion**](#Chapter5)\n",
    " - [**Chapter 6:  Limitations**](#Chapter6)\n",
    " - [**Chapter 7: Questions**](#Chapter7)\n",
    " - [**Chapter 8: References**](#Chapter8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Chapter0'></a>\n",
    "# Chapter 0: Introduction\n",
    "\n",
    "## Goal\n",
    " \n",
    "\n",
    "The goal of this notebook is to twofold: \n",
    "\n",
    " 1. Try to have a better understanding about the return risk-profile of the **value (HML) factor, momentum (WML) factor and a combined portfolio of value and momentum (HML-WML)** . I also include the **market factor (Mkt-RF)** as a sort of benchmark. For now, I've focused on **geometric return (annualized) and maximum drawdown**. In case of **maximum drawdown**, I am concerned whether it make sense to bootstrap this statistic, since this an extremum  statistic. I have seen paper bootstrapping this statistic (e.g.[**[1]**](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3331680)).\n",
    "\n",
    "2. Try to answer wether recent performance of value and momentum are in line with historical performance. Several people have noticed that most factors did not deliver much return in the last 15 years. Are the value and momentum broken or just experiencing a period of bad luck? \n",
    "\n",
    "## Statistical methods <a id='0.1'></a>\n",
    "To answer these question I use different simulation assumptions about the behavior of factor returns:\n",
    "\n",
    "\n",
    " - **Simulation under normality** approximates factor returns by normal distributions\n",
    "that I calibrate using the sample means and standard deviations from the full\n",
    "sample.\n",
    "\n",
    "\n",
    "- **The independent bootstrap** \n",
    "Resamples each factor’s return independently with replacement (i.e., allowing observations to\n",
    "be chosen multiple times) from every other factor. This bootstrap scheme\n",
    "preserves the empirical distributions of factor returns that is, it accounts for\n",
    "deviations from normality, but does not account for serial correlations in factor returns.\n",
    "\n",
    "\n",
    "- **Block bootstrap**: The independent bootstrap requires that the data being bootstrapped is independent and identically distributed (iid). This does not work well for time series, where serial correlation is present. One approach that addresses this limitation is block bootstrapping. There are several versions of the block bootstrap, I will look at the:  \n",
    "<br> \n",
    "    - **Circular block bootstrap (CBB)**: This bootstrap method is an improvement on the **moving block bootstrap (MBB)**. The MBB randomly draws fixed size blocks from the data and cut and pastes them to form a new series the same size as the original data. However, it has a major limitation in that beginning and ending points are systematically underrepresented. To address this limitation an extension to this method was developed called the Circular Block Bootstrap (CBB). This approach is much the same except that it wraps around the beginning and ending points to ensure they are drawn with similar probability as the other blocks To address this limitation an extension to this method was developed called the Circular Block Bootstrap (CBB). This approach is much the same except that it wraps around the beginning and ending points to ensure they are drawn with similar probability as the other blocks. \n",
    "<br>\n",
    "\n",
    "    - **Stationary bootstrap (SB)**: One limitation of the circular block bootstrap is the fixed block size. Different block sizes emphasize different periods or lengths of autocorrelation (memory). At the extremes you can take a block size so small that no serial correlation is captured, and at the other end you could take a block size so large that you end up sampling the original series. To address the fixed block sizes, the Stationary Bootstrap (SB) randomly samples blocks from a geometric distribution with mean $k$. The advantage of this approach is that the circular block bootstrap doesn't quite give us a stationary time series (The distribution of $R_{k-1:k}$ is not the same as the distribution of $R_{k:k+1}$). Averaging over the random choices of block lengths overcomes this problem.\n",
    "\n",
    "I found an excellent implementation of these methods from the [**ARCH package [13]**](https://arch.readthedocs.io/en/latest/bootstrap/timeseries-bootstraps.html). \n",
    "\n",
    "**Bootstrap implementation**\n",
    "\n",
    "When bootstrapping the simulated paths for different investment horizons, I use the total length of the observed path to sample from. As a result, I obtain a  matrix where each column is a simulated path. Similar as **[Fama and French [2]](https://academic.oup.com/raps/article-abstract/8/2/232/4810768)** I then calibrate the number of rows of this matrix depending on the investment horizon.\n",
    "\n",
    "\n",
    "## Data and portfolio construction <a id='0.2'></a>\n",
    "\n",
    "I will be using daily returns (possible to change to monthy) obtained from [**Kenneth French's data library [8]**](http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html) for the US stock market (NYSE, AMEX, and NASDAQ) over the period  03/01/1927 to 28/02/2019. The data can be sourced directly from their website using [**Pandas datareader [14]**](https://pandas-datareader.readthedocs.io/en/latest/remote_data.html). Note thate the Fama-French returns are simple raw returns (not log returns).\n",
    "\n",
    "**As a reminder:**\n",
    "\n",
    " - A **value strategy** favors high fundamentals-to-price ratios by going long these stocks, while selling short those stocks\n",
    "that have lower fundamentals-to-price ratios.\n",
    " - On the other hand, a **momentum strategy** exploits the phenomenon that securities which have performed well relative\n",
    "to peers (winners) on average continue to outperform, and securities that have performed relatively poorly (losers) tend to\n",
    "continue to under-perform.\n",
    "\n",
    "**Long-Short Portfolio Construction:**\n",
    "\n",
    "There are different ways to construct long-short value and momentum portfolio's. For now I will construct the portfolio's as Fama and French did in their 2012 [**paper [3]**](https://www.sciencedirect.com/science/article/pii/S0304405X12000931):\n",
    "\n",
    "  -  **Value Portfolio (HML):** is formed by first splitting the universe of stocks into two size categories: `Small (S)  and Big (B)` using NYSE market cap medians and then splitting stocks `into three groups based on book-to-market equity highest 30% (H), middle 40% (M), and lowest 30% (L)`, using NYSE breakpoints. The intersection of stocks across the six categories are value-weighed and used to form the portfolios SH (small, high $\\frac{BE}{ME}$), SM (small, middle $\\frac{BE}{ME}$), SL (small, low $\\frac{BE}{ME}$), BH (big, high $\\frac{BE}{ME}$), BM (big, middle $\\frac{BE}{ME}$), and BL (big, low $\\frac{BE}{ME}$). HML is the average of the two high book-to-market portfolios minus the average of the two low book-to-market portfolios. \n",
    "\n",
    "\\begin{equation} \\text{HML} =\t\\frac{1}{2}\\times  (\\text{Small High Value} + \\text{Big High Value})\n",
    " - \\frac{1}{2}\\times (\\text{Small Low Value} + \\text{Big Low Value})\\end{equation}\n",
    "\n",
    " - **Momentum portfolio (WML):** is constructed similarly to HML in which six value-weight portfolios formed on size and prior (2-12) returns to construct momentum. Specifically, prior (2-12) returns is defined as the past 12-month return, skipping the most recent months return to avoid micro-structure and liquidity biases and now generally used as the standard definition of momentum. The portfolios, which are formed daily, are the `intersections of 2 portfolios formed on size (market equity, ME) and 3 portfolios formed on prior (2-12) return`. The daily size breakpoint is the median NYSE market equity. The daily prior (2-12) return breakpoints are the 30th and 70th NYSE percentiles. Momentum is the average return on the two high prior return portfolios (Winners) minus the average return on the two low prior return portfolios (Losers)\n",
    "\n",
    "\\begin{equation} \\text{WML} =\t\\frac{1}{2}\\times  (\\text{Small Winners} + \\text{Big Winners})\n",
    " - \\frac{1}{2}\\times (\\text{Small Losers} + \\text{Big Losers})\\end{equation}\n",
    "\n",
    "\n",
    " - **Value-Momentum portfolio (HML-WML)**: is a `50-50 weighting` of the value and momentum strategy.\n",
    " \n",
    "\n",
    " -  **Market portfolio (Mkt-RF):** represents the equity market risk premium, or aggregate equity return minus the risk free\n",
    "rate. It is the return from simply being long equities at market capitalization weights and, unlike other factors, is not a\n",
    "spread return between\n",
    "\n",
    "Other common implementations of value and momentum are based on deciles and quintiles. For example, Moskowitz and Daniel [[4](https://www.sciencedirect.com/science/article/pii/S0304405X16301490)] construct their momentum portfolios based on deciles where the winner portfolio is decile 1 and the loser portfolio decile 10. Formally, this can be represented as follow:\n",
    "\n",
    "\\begin{equation} \\text{WML (Dec.)} =\t\\frac{1}{2}\\times\\text{Winners (D1)}\n",
    " - \\frac{1}{2}\\times \\text{Losers (D10)}\\end{equation}\n",
    "\n",
    "If I have time left, I will take a closer look at the influence of portfolio construction based on `deciles and quintiles` (I did had very quick look  at the implementation based on deciles, when verifying results of Kent and Moskowitz, and noticed that  the implementation based on `deciles` earns a substantial higher premium, but also much higher drawdowns)\n",
    "\n",
    "What follows is some of my initial experimentation with these simulation approaches along with some caveats. The analysis is organized as follow: I start with construction the portfolios in [**Chapter 1**](#Chapter1). Next, I introduce some common performance statistics and perform some exploratory data analysis in [**Chapter 2**](#Chatper2). In [**Chapter 3**](#Chapter3), I start with part 1 of the simulation analysis where I try to gain a better understanding of the risk-return profile for the portfolios under different investment horizons. Subsequently, in [**Chapter 4**](#Chapter4), I look at the recent past performance of these portfolios and examine whether this is in line with historical performance. Finally, in [**Chapter 5**](#Chapter5) and [**Chapter 6**](#Chapter6) , I summarize my conclusions and discusses limitations of the study. \n",
    "\n",
    "**I have stated my questions regarding the analysis in** [**Chapter 7**](#Chapter7). \n",
    "\n",
    "\n",
    "At the top of each chapter I have declared which functions are being used. I have decided to put these functions into a separate file named `simulation_lib` to keep this notebook as well-organized as possible. Furthermore, note that I use strategies, portfolios, factors interchangeably throughout the analysis. They all have the same interpretation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font size=\"5\">Import libraries</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:51:05.512324Z",
     "start_time": "2019-04-14T23:51:02.999046Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import datetime\n",
    "import os \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#os.chdir(\"C:\\\\Users\\Pieter-Jan\\Documents\\Factor_Crashes\\Poster\")\n",
    "\n",
    "# scientific libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy.stats.mstats import gmean\n",
    "import statsmodels.api as sm\n",
    "from pandas_datareader.famafrench import get_available_datasets\n",
    "import pandas_datareader.data as web\n",
    "from random import randint\n",
    "#import numba as nb\n",
    "#from numba import jit\n",
    "#from numba import jitclass\n",
    "\n",
    "\n",
    "# plot libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "#import brewer2mpl\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "# imports file with all custum functions\n",
    "from importlib import reload \n",
    "import simulation_lib\n",
    "reload(simulation_lib)\n",
    "# instead of listing all individual functions, we chose here to load everything\n",
    "# Make sure to use unique function names, though, or you might overrule built-in functions\n",
    "from simulation_lib import *\n",
    "\n",
    "# plot parameters\n",
    "sns_params = {\n",
    "    'font.family':'serif',\n",
    "    'font.size': 12,\n",
    "    'font.weight': 'medium',\n",
    "    'figure.figsize': (10, 7),\n",
    "}\n",
    "plt.style.use('seaborn-talk')\n",
    "plt.style.use('bmh')\n",
    "sns.set_context(sns_params)\n",
    "savefig_kwds = dict(dpi=300, bbox_inches='tight', frameon=True, format='png')\n",
    "set2=['#348ABD', '#A60628', '#7A68A6', '#467821', '#D55E00', '#CC79A7', '#56B4E9']\n",
    "#plt.style.available\n",
    "#sns.set(style='darkgrid', context='talk')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Chapter1'></a>\n",
    "# Chapter 1: Data and portfolio construction\n",
    "\n",
    "I start with constructing the four portfolios:\n",
    " - The market portfolio (Mkt-RF)\n",
    " - The value portfolio (HML)\n",
    " - The momentum portfolio (WML)\n",
    " - A 50-50 weighted value-momentum portfolio (HML-WML)\n",
    " \n",
    "I will implement the long-short portfolio myself using the raw portfolio sorts to walk through implementation process. Alternatively, you could directly take the long short portfolio constructed by Fama and French. To verify whether my portfolio implementation is correct, I compare my portfolio construction with Fama and French's implementation. Ideally, I should get the exact same results. This is only applicable for the HML and WML portfolio. For the market portfolio there is no sorting involved since this is just a value weighted portfolio of all stocks ((NYSE, AMEX, and NASDAQ)) minus the risk free. Finally, for the HML-WML portfolio, I choose arbitrarily a 50-50 weighting.\n",
    "\n",
    "### Helper function\n",
    " - `cumsum_plot:` plots the cumulative return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global parameters\n",
    "Global parameters are defined in CAPITAL LETTERS. This makes it is easy to quickly change from daily data to monthly and some other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:51:09.968930Z",
     "start_time": "2019-04-14T23:51:09.964940Z"
    }
   },
   "outputs": [],
   "source": [
    "# data frequency, easy to change between month and daily data\n",
    "FREQUENCY = 12\n",
    "N_PATHS = 10_000  # nr ob paths (bootstraps)\n",
    "BLOCKSIZE = 18  # blocksize, for the Stationary bootstrap this is the mean of the geometric distribution\n",
    "IH = [1, 3, 5, 10, 20, 30]  # investment horizon in years\n",
    "START_YEAR = 1927  # start year when sourcing the portfolios from FAMA AND FRENCH\n",
    "PLOT_INTERMEDIATE = False  # plots intermediate histograms when calling the simulation methods\n",
    "\n",
    "# Portfolio names from FAMA AND FRENCH ; to switch to monthly just remove '_daily'\n",
    "VALUE = '6_Portfolios_2x3'\n",
    "CHECK_VALUE = 'F-F_Research_Data_Factors'\n",
    "MOMENTUM = '6_Portfolios_ME_Prior_12_2'\n",
    "CHECK_MOMENTUM = 'F-F_Momentum_Factor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:51:16.200227Z",
     "start_time": "2019-04-14T23:51:14.577642Z"
    }
   },
   "outputs": [],
   "source": [
    "# get a look at all the available portfolios\n",
    "print(len(get_available_datasets()))\n",
    "get_available_datasets()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Minus Low (HML) portfolio <a id='1.1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:51:30.766836Z",
     "start_time": "2019-04-14T23:51:18.068929Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the data for the value factor\n",
    "dic_HML = web.DataReader(VALUE, 'famafrench', start=START_YEAR)\n",
    "print(dic_HML['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:51:45.678824Z",
     "start_time": "2019-04-14T23:51:44.281564Z"
    }
   },
   "outputs": [],
   "source": [
    "df_HML = dic_HML[0]\n",
    "df_HML.index = df_HML.index.astype('datetime64[ns]')\n",
    "print(df_HML.head(n=4))\n",
    "cumsum_plot(\n",
    "    df_HML,\n",
    "    title=\"Six Portfolio sorted on book-to-market (BM) and market equity (ME)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct HLM long short portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:51:45.701762Z",
     "start_time": "2019-04-14T23:51:45.683810Z"
    }
   },
   "outputs": [],
   "source": [
    "# long short value portfolio\n",
    "longs, shorts = ['SMALL HiBM', 'BIG HiBM'], ['SMALL LoBM', 'BIG LoBM']\n",
    "HML_LS = pd.DataFrame(df_HML.loc[:, longs].sum(axis=1) / 2 -\n",
    "                      df_HML.loc[:, shorts].sum(axis=1) / 2,\n",
    "                      columns=['HML_VW'])  # value weighted HML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:51:46.246305Z",
     "start_time": "2019-04-14T23:51:45.703757Z"
    }
   },
   "outputs": [],
   "source": [
    "cumsum_plot(HML_LS, title=\"Long-short value portfolio (HML)\")\n",
    "HML_LS.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare both HML long short portfolios\n",
    "If my construction method is correct, we should get exact same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:51:49.347012Z",
     "start_time": "2019-04-14T23:51:46.249300Z"
    }
   },
   "outputs": [],
   "source": [
    "dict_HML_check = web.DataReader(CHECK_VALUE, 'famafrench', start=START_YEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:51:49.367955Z",
     "start_time": "2019-04-14T23:51:49.352998Z"
    }
   },
   "outputs": [],
   "source": [
    "print(dict_HML_check['DESCR'])\n",
    "HML_check = dict_HML_check[0]\n",
    "HML_check.index = HML_check.index.astype('datetime64[ns]')\n",
    "HML_check.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:51:49.937433Z",
     "start_time": "2019-04-14T23:51:49.369951Z"
    }
   },
   "outputs": [],
   "source": [
    "Compare_HML = pd.DataFrame(HML_check.loc[:, \"HML\"].values,\n",
    "                           columns=[\"HML_1\"],\n",
    "                           index=HML_check.index)\n",
    "Compare_HML[\"HML_2\"] = HML_LS.loc[:, \"HML_VW\"].values\n",
    "cumsum_plot(Compare_HML,\n",
    "            title=\"Comparison of the value portfolio implementation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems identical to me. Now let's do the same for the momentum factor (WML)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Winners Minus Losers (WML) portfolio <a id='1.2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:51:55.146500Z",
     "start_time": "2019-04-14T23:51:49.938431Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the data for the value factor\n",
    "dic_WML = web.DataReader(MOMENTUM, 'famafrench', start=START_YEAR)\n",
    "print(dic_WML['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:51:55.159466Z",
     "start_time": "2019-04-14T23:51:55.148495Z"
    }
   },
   "outputs": [],
   "source": [
    "df_WML = dic_WML[0]\n",
    "df_WML.index = df_WML.index.astype('datetime64[ns]')\n",
    "df_WML.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:51:56.759187Z",
     "start_time": "2019-04-14T23:51:55.164452Z"
    }
   },
   "outputs": [],
   "source": [
    "cumsum_plot(\n",
    "    df_WML,\n",
    "    title=\"Six Portfolio sorted on prior returns (2-12) and market equity (ME)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct WML long short portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:51:57.277800Z",
     "start_time": "2019-04-14T23:51:56.766173Z"
    }
   },
   "outputs": [],
   "source": [
    "# long short momentum portfolio\n",
    "longs, shorts = ['SMALL HiPRIOR',\n",
    "                 'BIG HiPRIOR'], ['SMALL LoPRIOR', 'BIG LoPRIOR']\n",
    "WML_LS = pd.DataFrame(df_WML.loc[:, longs].sum(axis=1) / 2 -\n",
    "                      df_WML.loc[:, shorts].sum(axis=1) / 2,\n",
    "                      columns=['WML_VW'])  # momemtum weighted HML\n",
    "# plot the cumulative return\n",
    "cumsum_plot(WML_LS, title=\"Long-short momentum portfolio (WML)\")\n",
    "WML_LS.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:51:59.820997Z",
     "start_time": "2019-04-14T23:51:57.281789Z"
    }
   },
   "outputs": [],
   "source": [
    "dict_WML_check = web.DataReader(CHECK_MOMENTUM, 'famafrench', start=START_YEAR)\n",
    "print(dict_WML_check['DESCR'])\n",
    "WML_check = dict_WML_check[0]\n",
    "WML_check.index = WML_check.index.astype('datetime64[ns]')\n",
    "WML_check.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare both WML long short portfolios\n",
    " - if my construction method is correct this should be exactly the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:52:00.541071Z",
     "start_time": "2019-04-14T23:51:59.822993Z"
    }
   },
   "outputs": [],
   "source": [
    "Compare_WML = pd.DataFrame(WML_check.loc[:, \"Mom   \"].values,\n",
    "                           columns=[\"WML_1\"],\n",
    "                           index=WML_check.index)\n",
    "\n",
    "Compare_WML[\"WML_2\"] = WML_LS.loc[:, \"WML_VW\"].values\n",
    "cumsum_plot(Compare_WML,\n",
    "            title=\"Comparison of the value portfolio implementation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50-50 Value Momemtum Portfolio <a id='1.3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:52:01.195322Z",
     "start_time": "2019-04-14T23:52:00.544065Z"
    }
   },
   "outputs": [],
   "source": [
    "w_HML, w_WML = 0.50, 0.50  #weights 50-50 weights (arbitrarely)\n",
    "\n",
    "HML_WML = pd.DataFrame(HML_check.loc[:, [\"HML\"]].values * w_HML +\n",
    "                       WML_check.loc[:, [\"Mom   \"]].values * w_WML,\n",
    "                       columns=[\"HML-WML\"],\n",
    "                       index=WML_check.index)\n",
    "\n",
    "cumsum_plot(HML_WML, title=\"Long-short value-momentum portfolio (HML-WML)\")\n",
    "HML_WML.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Market Portfolio <a id='1.4'></a>\n",
    "\n",
    "This is just the market (Mkt) portfolio minus the risk free rate (RF). I use this as the benchmark since basically every investor (including you and me) can buy this portfolio for almost no fee (there is no performance fee involved). You can think of this portfolio as the **[S&P500](https://en.wikipedia.org/wiki/S%26P_500_Index)**, but containing many more firms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:52:01.747843Z",
     "start_time": "2019-04-14T23:52:01.197316Z"
    }
   },
   "outputs": [],
   "source": [
    "cumsum_plot(pd.DataFrame(HML_check.loc[:, \"Mkt-RF\"], columns=[\"Mkt-RF\"]),\n",
    "            title=\"Market portfolio minus the risk free rate (Mkt-RF)\")\n",
    "HML_check.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare 4 factors\n",
    "   - Market factor (Mkt-RF)\n",
    "   - Value factor (HML)\n",
    "   - Momemtum factor (WML)\n",
    "   - 50-50 value-momemtum portfolio (HML-WML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:52:02.647440Z",
     "start_time": "2019-04-14T23:52:01.748841Z"
    }
   },
   "outputs": [],
   "source": [
    "# append all four portfolios together in one dataframe\n",
    "df_factors = pd.DataFrame(HML_check.loc[:, [\"Mkt-RF\", \"HML\"]])\n",
    "df_factors[\"WML\"] = WML_check.iloc[:, 0]\n",
    "df_factors[\"HML-WML\"] = HML_WML.iloc[:, 0]\n",
    "\n",
    "# alternative legend labels\n",
    "dic_fac = {\n",
    "    \"Mkt-RF\": \"Market minus risk free rate (Mkt-RF)\",\n",
    "    \"HML\": \"Value (HML)\",\n",
    "    \"WML\": \"Momentum (WML)\",\n",
    "    \"HML-WML\": \"50-50 Momentum-Value (50-50 HML-WML)\"\n",
    "}\n",
    "\n",
    "# plot\n",
    "cumsum_plot(df_factors, dic=dic_fac, title=\"Portfolio comparison\")\n",
    "df_factors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Chapter2'></a>\n",
    "# Chapter 2:  Risk- risk reward ratio's and Data exploration\n",
    "\n",
    "Before starting with the simulation exercise, I introduce some performance statistics and perform some data exploration. The goal is to gain insights that give us intuition about the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return and risk reward ratio's <a id='2.1'></a>\n",
    "\n",
    "**In case when working with log returns:** \n",
    "\n",
    "- I was considering to change to log returns for computational speed, but then decided it was not worth the effort (maybe I will take another look)\n",
    "\n",
    "The **geometric mean** of quantities $\\left\\{a_{1}, \\dots, a_{n}\\right\\}$ is\n",
    "\n",
    "\\begin{equation}\n",
    "\\overline{a}_{g}=\\left(\\prod_{i=1}^{n} a_{i}\\right)^{1 / n}\n",
    "\\end{equation}\n",
    "\n",
    "Taking the logarithm of both sides gives\n",
    "\\begin{equation}\n",
    "\\log \\overline{a}_{g}=\\frac{1}{n} \\sum_{i=1}^{n} \\log a_{i}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "so the log of the geometric mean is equal to the arithmetic mean of the logs.\n",
    "In  case, the relevant quantities $a_i$ are the growth rates over each period\n",
    "\n",
    "\\begin{equation}\n",
    "a_{i}=1+r_{i}\n",
    "\\end{equation}\n",
    "\n",
    "and plugging this into the above equation gives\n",
    "\n",
    "\\begin{equation}\n",
    "\\log \\left(1+\\overline{r}_{g}\\right)=\\frac{1}{n} \\sum_{i=1}^{n} \\log \\left(1+r_{i}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\overline{r}_{g}=\\exp \\left(\\frac{1}{n} \\sum_{i=1}^{n} \\log \\left(1+r_{i}\\right)\\right)-1\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Sometimes the geomtric return is  approximated by the following formula:\n",
    "\n",
    "\\begin{equation}\n",
    "\\overline{r}_{g}=\\frac{1}{n} \\sum_{i=1}^{n} \\log \\left(1+r_{i}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "this is because when the growth rate is small,\n",
    "\n",
    "\\begin{equation}\n",
    "\\log \\left(1+\\overline{r}_{g}\\right) \\approx \\overline{r}_{g}\n",
    "\\end{equation}\n",
    "\n",
    "it is often an approximation that you can get away with\n",
    "\n",
    "**Simple raw returns**\n",
    "\n",
    "- **Arithmetic return**: \\begin{equation} R_{a} =\n",
    "\\frac{1}{n} \\sum_{i=1}^{n} r_{i}=\\frac{r_{1}+r_{2}+\\cdots+r_{n}}{n}\n",
    "\\end{equation}\n",
    "\n",
    "suppose our arithmetic return is at daily frequency, to annualize this I take $(R_a)\\times 252$ (assuming 252 trading days in a year)\n",
    "\n",
    "- **Geometric return**: \\begin{equation} R_{g} = \n",
    "\\sqrt[n]{\\left(1+r_{1}\\right) \\times\\left(1+r_{2}\\right) \\times \\ldots\\left(1+r_{n}\\right)}\n",
    "\\end{equation}\n",
    "\n",
    "suppose our geometric return is at daily frequency, to annualize this I take $(R_g)^{252} - 1$ (assuming 252 trading days in a year)\n",
    "\n",
    "\n",
    "- **Sharpe ratio**: one of the most common risk-reward ratios used in finance is the Sharpe ratio. The Sharpe ratio discounts the expected excess returns of a portfolio $ {E(r)-r_{f}} $ by the volatility of the returns $\\sigma_r$. Formally, this can be described as follow:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "S=\\frac{E\\left(r_{e}\\right)}{\\sigma_{r}}=\\frac{E(r)-r_{f}}{\\sigma_{r}}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\sigma=\\sqrt{\\sigma^{2}}=\\sqrt{\\frac{1}{N} \\sum_{i=1}^{N}\\left(r_{i}-\\mu\\right)^{2}} \\text {and } \\mu=\\frac{1}{N} \\sum_{i=1}^{N} r_{i}$\n",
    "\n",
    "**Crude estimation of annualized volatility**\n",
    "\n",
    "Most often volatility is annualized by multiplying by $\\sqrt(n)$ where $n$ represents the time period (number of trading days or trading months). The formula uses the the fact that the standard deviation of the sum of $n$ independent variables (with equal standard deviations) is $\\sqrt(n)$ times the standard deviation of the individual variables. This is only a crude approximation of the true volatility observed in financial markets and is only an accurate extrapolation if prices follow a random walk or Wiener process, whose steps have finite variance. As a result we will probably underestimate volatility. There are other ways (probably better) to measure volatility, but these are not the focus of this project.\n",
    "\n",
    "**Lower Partial Moments (LPM)**\n",
    "\n",
    "Whereas measures of risk-adjusted return based on volatility treat all deviations from the mean as risk, measures of risk-adjusted return based on lower partial moments  consider only deviations below some predefined minimum return threshold, $\\tau$ as risk. For example, negative deviations from the mean is risky whereas positive deviations are not. A lower partial moment of order $j$ can be estimated from a sample of $k$ returns as follows: \\begin{equation}\n",
    "L P M_{j}(\\tau)=\\frac{1}{k} \\sum_{i=1}^{k} \\max \\left(\\tau-r_{i}, 0\\right)^{j} \\end{equation}\n",
    "\n",
    "\n",
    "A useful classification of measures of risk-adjusted returns based on lower partial moments is by their order. The larger the order the greater the weighting will be on returns that fall below the target threshold, meaning that larger orders result in more risk-averse measures. Some popular measures using the equation above are:\n",
    "\n",
    "   - **The Sortino ratio** was proposed as a modification to the Sharpe ratio. It discounts the excess return of a portfolio above a target threshold by the volatility of downside returns, $\\delta^{2}$, instead of the volatility of all returns, $\\sigma^2$. The volatility of downside returns is equivalent to the square-root second-order lower partial moment of returns. More formally this can be described as follow:\n",
    "   \n",
    "   \\begin{equation}\n",
    "S O R(\\tau)=\\frac{E\\left(r_{e}\\right)}{\\delta^{2}}=\\frac{E(r)-\\tau}{\\delta^{2}}=\\frac{E(r)-\\tau}{\\sqrt{L P M_{2}(\\tau)}}\n",
    "\\end{equation}\n",
    "   \n",
    "\n",
    "   - **The Omega ratio** discounts the excess returns of a portfolio above the target threshold, usually the risk-free rate, by the first-order lower partial moment of the returns. The first-order lower partial moment corresponds to the average expected loss also known as downside risk. Mathematically the omega ratio is defined as follow:\n",
    "   \n",
    "   \\begin{equation}\n",
    "\\Omega(\\tau)=\\frac{E\\left(r_{e}\\right)}{L P M_{1(\\tau)}}=\\frac{E(r)-\\tau}{L P M_{1}(\\tau)}\n",
    "\\end{equation}\n",
    "   \n",
    "\n",
    "   - **The Kappa ratio** is a generalization of Omega and Sortino ratio first proposed in 2004 by Kaplan and Knowles.  It was shown that when the parameter $j$ of the Kappa ratio is set to one or two you get the Omega or Sortino ratio. The Kappa ratio is most often used with $j=3$ which is why it is often referred to as the Kappa 3 ratio. Mathematically this is defined as follow:\n",
    "   \n",
    "   \\begin{equation}\n",
    "K_{j}(\\tau)=\\frac{E\\left(r_{e}\\right)}{\\sqrt[j]{L P M_{j}(\\tau)}}=\\frac{E(r)-\\tau}{\\sqrt[j]{L P M_{j}(\\tau)}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Another measure to look at is risk is **Drawdown**.  It is the maximum decrease in the value of the portfolio over a specific period of time. Given the historical prices (values) for a portfolio, $S$, and a period of time, $t$, the drawdown of length $t$ over that period of time is the maximum distance between a previous two values $S_x$ and $S_{x−t}$,\n",
    "   \n",
    "   \\begin{equation}\n",
    "D(t)=\\max \\left\\{0, \\max _{t_{i} \\in(0, t}\\left\\{S_{t_{i}}-S_{t_{i}-t}\\right\\}\\right\\}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "**Maximum drawdown** can be thought of as a list of drawdowns calculated from the same historical portfolio values, $S$, but for the whole time period (or for different sub time periods, rolling window). The maximum drawdown of a portfolio is the maximum decrease in portfolio value from a previous high to a new low.\n",
    "\n",
    "**% Time underwater** is the percentage time the investment is in in a drawdown state. \n",
    "\n",
    "Personally, I like measures based on drawdowns since they are extremely intuitive and are IMO are a very natural way to look at risk. Contrary to other measures, drawdown is path dependent. Furthermore, I very much enjoyed the presentation of Rober Frey (Former Quant of the Medallion fund) on drawdown [**180 years of market drawdown [8]**](https://www.youtube.com/watch?v=27x632vOjXk&t=3349s).\n",
    "\n",
    "### Helper functions\n",
    " - `Kappa_ratio:` calculates the kappa ratio \n",
    " - `drawdown:` calculates drawdown\n",
    " - `Anonymous functions (lambda):` performance functions stored in a dictionary\n",
    " -  `perf_stats:` a wrapper around the anonymous functions to give the possibility for selecting the function(s) of interest\n",
    " - `Return_DD:`calculates the cumulative return and drawdown in one figure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:52:02.701295Z",
     "start_time": "2019-04-14T23:52:02.682344Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "some anonymous function to calculate various (performance) statistics,\n",
    "these functions are stored in a dictionary\n",
    "\n",
    "params:\n",
    "\n",
    "    - df: pandas dataframe with the raw returns\n",
    "    - time_freq: an integer indicating the number of trading days\n",
    "    - treshold: a float determining the level of drawdown is taken into account\n",
    "      when calculating the percentage of time an investment is in a drawdown state\n",
    "\n",
    "returns:\n",
    "\n",
    "    - numpy array with the calculted statistic\n",
    "\"\"\"\n",
    "\n",
    "P_AR = lambda df, time_freq=FREQUENCY: np.mean(df) * time_freq\n",
    "P_GR = lambda df, time_freq=FREQUENCY: (gmean((1 + df / 100))**time_freq - 1\n",
    "                                        ) * 100\n",
    "P_AVol = lambda df, time_freq=FREQUENCY: df.std() * np.sqrt(time_freq)\n",
    "P_GVol = lambda df, time_freq=FREQUENCY: (np.exp(np.std(np.log(1 + df / 100)))\n",
    "                                          - 1) * np.sqrt(time_freq) * 100\n",
    "\n",
    "# put functions in a dictionary, so we can choose which one to call and\n",
    "# which not (for the simulation)\n",
    "perf_stats_dic = {\n",
    "    \"Skew\": (lambda df: df.skew().values),\n",
    "    \"Kurtosis\": (lambda df: df.kurtosis().values),\n",
    "    \"Terminal Wealth Relative\":\n",
    "    (lambda df: (1 + df / 100).cumprod().iloc[-1, :] - 1),\n",
    "    \"Arithmetric Return (%)\":\n",
    "    (lambda df, time_freq=FREQUENCY: P_AR(df, time_freq).values),\n",
    "    \"Geometric Return (%)\":\n",
    "    (lambda df, time_freq=FREQUENCY: P_GR(df, time_freq)),\n",
    "    \"Arithmetric Volatility (%)\":\n",
    "    (lambda df, time_freq=FREQUENCY: P_AVol(df, time_freq).values),\n",
    "    \"Geometric Volatility (%)\":\n",
    "    (lambda df, time_freq=FREQUENCY: P_GVol(df, time_freq)),\n",
    "    \"Arithmetric SR\": (lambda df, time_freq=FREQUENCY: P_AR(df, time_freq) /\n",
    "                       P_AVol(df, time_freq)),\n",
    "    \"Geometric SR\": (lambda df, time_freq=FREQUENCY: P_GR(df, time_freq) /\n",
    "                     P_GVol(df, time_freq)),\n",
    "    \"Maximum Drawdown (%)\": (lambda df: drawdown(df).max().values * 100),\n",
    "    \"Underwater (%)\": (lambda df: (drawdown(df) != 0).sum() / len(df) * 100),\n",
    "    f\"(%) Underwater >{20} %\":\n",
    "    (lambda df, treshold=0.20: (drawdown(df) > treshold).sum() / len(df) * 100)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA) <a id='2.2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:52:12.015381Z",
     "start_time": "2019-04-14T23:52:02.750163Z"
    }
   },
   "outputs": [],
   "source": [
    "g = sns.pairplot(df_factors, height=5, aspect=1.5, diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full sample period: 1927:2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:52:13.908319Z",
     "start_time": "2019-04-14T23:52:12.019371Z"
    }
   },
   "outputs": [],
   "source": [
    "Return_DD(df=df_factors, dic_labels=dic_fac, loc='upper center',time_frequency=FREQUENCY)\n",
    "# plt.savefig('Cum_return_DD', bbox_inches='tight') # save plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Period: 1927-1935\n",
    " - crash of 1930"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:52:15.253719Z",
     "start_time": "2019-04-14T23:52:13.910313Z"
    }
   },
   "outputs": [],
   "source": [
    "start, end = datetime.date(year=1927, month=1, day=1), datetime.date(year=1935,\n",
    "                                                                     month=1,\n",
    "                                                                     day=1)\n",
    "Return_DD(df=df_factors,\n",
    "          dic_labels=dic_fac,\n",
    "          loc='upper left',\n",
    "          start=start,\n",
    "          end=end,\n",
    "          time_frequency=FREQUENCY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Period: 1998-2010\n",
    " - chrash of 2000 with the internet buble\n",
    " - chrash of 2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:52:16.651981Z",
     "start_time": "2019-04-14T23:52:15.254718Z"
    }
   },
   "outputs": [],
   "source": [
    "start, end = datetime.date(year=1998, month=1, day=1), datetime.date(year=2010,\n",
    "                                                                     month=1,\n",
    "                                                                     day=1)\n",
    "Return_DD(df=df_factors,\n",
    "          dic_labels=dic_fac,\n",
    "          loc='upper center',\n",
    "          start=start,\n",
    "          end=end,\n",
    "          time_frequency=FREQUENCY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all serious drawdown periods, momentum crashes when the market starts rebouncing. This is in line what's described in the literature. Also note the negative correlation between value and momentum and momentum and the market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:52:17.021991Z",
     "start_time": "2019-04-14T23:52:16.653974Z"
    }
   },
   "outputs": [],
   "source": [
    "dic = {i: kappa_ratio(df_factors, order=i, time_frequency=FREQUENCY) for i in (np.arange(1, 6.5, 0.25))}\n",
    "out = pd.DataFrame(dic).T\n",
    "out = out.rename(columns={0: \"Mkt-RF\", 1: \"HML\", 2: \"WML\", 3: \"HML-WML\"})\n",
    "fig = figsize = (15, 8)\n",
    "fig, (ax1) = plt.subplots(1, 1, figsize=(15, 6))\n",
    "for i, c in enumerate(out.columns):\n",
    "    ax1.plot(out.index, out[c], label=str(c), linestyle=\"--\")\n",
    "    ax1.set_xlabel(\"$j$\")\n",
    "    ax1.set_ylabel(\"Kappa Ratio\")\n",
    "    ax1.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the power ($j$) of the `kappa ratio` gives higher weight to deviations below the treshold ($\\tau$=0). It is interesting to see that the  momenetum strategy intially has a higher kappa ratio, but get surpassed around $j=3$ by the value portfolio. This means the momentum portfolio contains some large negative returns (tail events)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:52:17.082827Z",
     "start_time": "2019-04-14T23:52:17.022988Z"
    }
   },
   "outputs": [],
   "source": [
    "round(perf_stat(df=df_factors, dic_perf=perf_stats_dic , stats=\"all\"), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:52:17.117734Z",
     "start_time": "2019-04-14T23:52:17.083825Z"
    }
   },
   "outputs": [],
   "source": [
    "round(df_factors.describe(), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some observations with respect to performance assessment of the four portfolios**\n",
    "\n",
    "- Momentum seems to be the more \"risky\" strategy when looking at `skewness, kurtosis, kappa ratio` though it has not the highest `drawdown`. Note that the Sharpe ratio does not reveal this sort of information ( IMO, it does not make much sense to look at the Sharpe ratio when returns are not normally distributed).\n",
    "\n",
    "\n",
    "- **Around 93%** of the time, the portfolios are in a state of drawdown (a state of regret). Meaning if we pick a random day, this is very high likelihood, there is some point in time we had more money. I think this an underappreciated fact and has a big impact on emotional state of investors. Note, this is not something specific for value or momentum investing. It is a feature of most investment strategies (even very good ones, see [**180 years of market drawdown [8]**](https://www.youtube.com/watch?v=27x632vOjXk&t=3349s)).\n",
    "\n",
    "\n",
    "- Big differences between the four portfolios when looking at the `(%) underwater > 20 %` (I kind of arbitrarily chose 20 % to indicate a big drawdown). The difference is espcially big between the value and momentum strategy (43% vs 17%). Momentum experiences two very big drawdown period 1930-1960 and 2009-today.\n",
    "\n",
    "\n",
    "- Combining value and momentum (HML-WML) improves risk adjusted performance on almost all statistics, though this relation is time dependent and seems to have disappeared over the last decade.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:52:29.978337Z",
     "start_time": "2019-04-14T23:52:17.118732Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Returns\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 8))\n",
    "axs = axs.ravel()\n",
    "for i, j in enumerate(df_factors.columns):\n",
    "    axs[i].scatter(df_factors.index,\n",
    "                   df_factors.iloc[:, i].values,\n",
    "                   label=j,\n",
    "                   color=set2[i],\n",
    "                   s=8)\n",
    "    axs[i].set_xlabel(\"Date\")\n",
    "    axs[i].set_ylabel(\"Raw returns\")\n",
    "    axs[i].legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Returns', y=1.05, fontsize=22, fontweight=\"bold\")\n",
    "\n",
    "# Squared returns\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 8))\n",
    "axs = axs.ravel()\n",
    "for i, j in enumerate(df_factors.columns):\n",
    "    axs[i].scatter(df_factors.index,\n",
    "                   df_factors.iloc[:, i].values**2,\n",
    "                   label=j,\n",
    "                   color=set2[i],\n",
    "                   s=8)\n",
    "    axs[i].set_xlabel(\"Date\")\n",
    "    axs[i].set_ylabel(\"Squared returns\")\n",
    "    axs[i].legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Squared returns', y=1.05, fontsize=22, fontweight=\"bold\")\n",
    "\n",
    "# Partial autocorrelation: returns\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 8))\n",
    "axs = axs.ravel()\n",
    "for i, j in enumerate(df_factors.columns):\n",
    "    plot_pacf(df_factors.iloc[:, i].values,\n",
    "              ax=axs[i],\n",
    "              label=j,\n",
    "              lags=25,\n",
    "              color=set2[i])\n",
    "    axs[i].set_xlabel(\"Lags\")\n",
    "    axs[i].set_ylabel('$\\\\rho$')\n",
    "    axs[i].set_title(\"\")\n",
    "    #axs[i].set_ylim(-0.5,0.5)\n",
    "    handles, labels = axs[i].get_legend_handles_labels()\n",
    "    handles = handles[:-len(handles) // 3][0::1]\n",
    "    labels = labels[:-len(handles) // 3][0::1]\n",
    "    axs[i].legend(handles=handles, labels=labels, loc='best')\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Partial autocorrelation raw returns',\n",
    "             y=1.05,\n",
    "             fontsize=22,\n",
    "             fontweight=\"bold\")\n",
    "\n",
    "# Partial autocorrelation: squared returns\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 8))\n",
    "axs = axs.ravel()\n",
    "for i, j in enumerate(df_factors.columns):\n",
    "    plot_pacf(abs(df_factors.iloc[:, i]).values,\n",
    "              ax=axs[i],\n",
    "              label=j,\n",
    "              lags=25,\n",
    "              color=set2[i])\n",
    "    axs[i].set_xlabel(\"Lags\")\n",
    "    axs[i].set_ylabel('$\\\\rho$')\n",
    "    axs[i].set_title(\"\")\n",
    "    #axs[i].set_ylim(-0.5,0.5)\n",
    "    handles, labels = axs[i].get_legend_handles_labels()\n",
    "    handles = handles[:-len(handles) // 3][0::1]\n",
    "    labels = labels[:-len(handles) // 3][0::1]\n",
    "    axs[i].legend(handles=handles, labels=labels, loc='best')\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Partial autocorrelation absolute returns',\n",
    "             y=1.05,\n",
    "             fontsize=22,\n",
    "             fontweight=\"bold\")\n",
    "\n",
    "# Partial autocorrelation: squared returns\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 8))\n",
    "axs = axs.ravel()\n",
    "for i, j in enumerate(df_factors.columns):\n",
    "    plot_pacf(df_factors.iloc[:, i].values**2,\n",
    "              ax=axs[i],\n",
    "              label=j,\n",
    "              lags=25,\n",
    "              color=set2[i])\n",
    "    axs[i].set_xlabel(\"Lags\")\n",
    "    axs[i].set_ylabel('$\\\\rho$')\n",
    "    axs[i].set_title(\"\")\n",
    "    #axs[i].set_ylim(-0.5,0.5)\n",
    "    handles, labels = axs[i].get_legend_handles_labels()\n",
    "    handles = handles[:-len(handles) // 3][0::1]\n",
    "    labels = labels[:-len(handles) // 3][0::1]\n",
    "    axs[i].legend(handles=handles, labels=labels, loc='best')\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Partial autocorrelation squared returns',\n",
    "             y=1.05,\n",
    "             fontsize=22,\n",
    "             fontweight=\"bold\")\n",
    "\n",
    "# QQplot: simple returns\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 8))\n",
    "axs = axs.ravel()\n",
    "for i, j in enumerate(df_factors.columns):\n",
    "    sm.qqplot(df_factors.iloc[:, i].values,\n",
    "              fit=True,\n",
    "              line='45',\n",
    "              label=j,\n",
    "              ax=axs[i],\n",
    "              color=set2[i])\n",
    "    axs[i].legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Simple returns', y=1.05, fontsize=22, fontweight=\"bold\")\n",
    "\n",
    "# QQplot: log returns\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 8))\n",
    "axs = axs.ravel()\n",
    "for i, j in enumerate(df_factors.columns):\n",
    "    sm.qqplot(np.exp(1 + df_factors.iloc[:, i].values / 100),\n",
    "              fit=True,\n",
    "              line='45',\n",
    "              label=j,\n",
    "              ax=axs[i],\n",
    "              color=set2[i])\n",
    "    axs[i].legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Log returns', y=1.05, fontsize=22, fontweight=\"bold\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**some observations (all strategies)**\n",
    "\n",
    " - Returns are hetroskedastic:  return volatility is high during the 1930s, and one can argue that this period is a different regime, unlikely to repeat (some papers exclude the period 1927-1963 because of this, I personally disagree with this). In contrast, 1941–1962 is a period of relatively low return volatility. Thereafter, we observe similar behavior as in the 1930's.\n",
    " - Serial correlations in the simple returns is rather low, but much higher serial correlations in the absolute and squared returns (I am quite surprised, this is promising for the second part of the thesis).\n",
    " - Return are not normally distributed (simple and log)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:52:30.930817Z",
     "start_time": "2019-04-14T23:52:29.980332Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cumulative squared returns\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 8))\n",
    "axs = axs.ravel()\n",
    "for i, j in enumerate(df_factors.columns):\n",
    "    axs[i].plot(df_factors.index, (df_factors.iloc[:, i]**2).cumsum(),\n",
    "                label=j,\n",
    "                color=set2[i],\n",
    "                lw=4)\n",
    "    axs[i].legend(loc=\"best\")\n",
    "    axs[i].set_ylabel(\"CSR\")\n",
    "    axs[i].set_xlabel(\"Date\")\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Cumulative squared returns (CSR)',\n",
    "             y=1.05,\n",
    "             fontsize=22,\n",
    "             fontweight=\"bold\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cumulative Squared Returns (CSR)**\n",
    "\n",
    "The CSR is a “quick and dirty” approach to look at volatility regimes. The variance, $\\sigma_r^2$, of return can be expressed by the following, where $R$  is the random variable representing return:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma_{r}^{2}=\\mathrm{E}\\left[R^{2}\\right]-\\mathrm{E}[R]^{2}\n",
    "\\end{equation}\n",
    "\n",
    "If we take the maximum likelihood estimate (MLE) for the sample variance, $\\hat{s}_r^2$ , and multiply both sides by the sample size, $n$ , we have\n",
    "\n",
    "\\begin{equation}\n",
    "n \\hat{s}_{r}^{2}=\\sum_{t=1}^{n} r(t)^{2}-\\frac{\\left(\\sum_{t=1}^{n} r(t)\\right)^{2}}{n}\n",
    "\\end{equation}\n",
    "\n",
    "For most return time series the term to the left of the minus sign is much larger than the term on the right. This leads to the following approximation:\n",
    "\n",
    "\\begin{equation}\n",
    "n \\hat{s}_{r}^{2} \\simeq \\sum_{t=1}^{n} r(t)^{2}\n",
    "\\end{equation}\n",
    "\n",
    "Thus, when plotting the cumulative square returns (CSR), the slope of the CSR line is approximately the variance of those returns. If the return switches from one stable volatility regime to another, then this shift shows as an abrupt change in slope of the CSR. This is a simple but powerful exploratory data analysis technique. The idea is to gain insights that give intuition about the data. Of course, you want to make sure that our insights make sense, but at this stage I am not super concerned with the formalities.\n",
    "\n",
    "\n",
    "Looking at the CSR it does appear that a piecewise model yields a reasonably good description. There are extended periods where the slope is roughly constant, and these are separated by definite knits showing rapid transition from one regime to another.\n",
    "\n",
    "**Next steps (difficult in python)**\n",
    "\n",
    "My next step was to approximate the CSR with a piecewise linear model and look how these slopes change over time. The slope of the $i^{th}$ line segment ($\\tau$)  would  the estimate of the constant variance. If we multiple each variance by $12$ or $252$ (depending if you use monthly or daily data) and then take the square root, then we have an annualized estimate of the standard deviation:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{S}_{\\text { annual }}\\left(\\tau_{i}\\right)=\\sqrt{252 \\times \\hat{s}_{\\text { daily }}\\left(\\tau_{i}\\right)^{2}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Doing the above in the **R programming language** seems a lot more practical (there are some packages which identify changing points). So far, I have experimented a bit with this, the problem is determining the cutpoints to fit the piecewise model (a tried with a simple decidision tree which gave actually nice results). If I have time left, I will take another look at this. Suggestions are also welcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Chapter3'></a>\n",
    "\n",
    "# Chapter 3: Simulation analysis Part 1\n",
    "\n",
    "It seems likely that the characteristics of long-horizon returns are of central interest to investors saving for distant payoffs. If so, it is likely that the characteristics of long-horizon returns affect asset pricing in ways missed by traditionel models. Evidence on distributions of long-horizon returns is a logical first\n",
    "step in remedying this deficiency. I use simulations to study distributions of U.S. stock returns the distributions of annual, and 3-, 5-, 10-, 20-, and 30-year returns on the  Market portfolio (Mkt-RF), value portfolio (HML), a momentum portfolio (WML)  and a 50-50 value-momentum portfolio (HML-WML) of U.S. stocks for January 1927 to February 2019 (henceforth 1927–2019). Sample sizes shrink quickly as the return horizon increases. I use (bootstrap) simulations to fill in details of long horizon return distributions. \n",
    "\n",
    "**Portfolios**\n",
    "\n",
    "- market (Mkt-RF)\n",
    "- Value (HML)\n",
    "- Momentum (WML)\n",
    "- 50-50 Value-Momentum (HML-WML) \n",
    "\n",
    "**simulations methods**\n",
    "\n",
    "   - sample returns from a normal distributions using the $\\mu$ and $\\sigma$ estimated from the data\n",
    "   - Independent identical distributed (i.i.d.) bootstrap\n",
    "   - circular bootstrap\n",
    "   - stationairy bootstrap\n",
    "\n",
    "**statistics of interest**\n",
    "\n",
    "I focus on the following three statistics which I consider important elements of any investment strategy:\n",
    " \n",
    " - geometric return (annualized) \n",
    " - the number of times we end up with a positive return (based on geometric return)\n",
    " - Maximum Drawdown\n",
    " \n",
    "Regarding the maximum drawdown, I am concerned whether this makes sense, since this is an extrema statistic.\n",
    "\n",
    "**Investment horizon**\n",
    "\n",
    "- 1-3-5-10-20-30 years\n",
    "\n",
    "Regarding **geometric return**, I expect it to converge toward a normal distributions (this is a rather simple and well behaved statistic) as the investment horizon increases. I'm most of all curious about how fast the variance will drop as you increase the investment horizon.\n",
    "\n",
    "Regarding **maximum drawdown**, I honestly don't know what to expect (maybe some kind of right skewed distribution?). As you increase the investment horizon you will experience larger maximum drawdown by construction (your biggest loss is in the future). Does it even makes sense to look at such a statistic in a bootstrap setting?\n",
    "\n",
    "### Helper functions\n",
    "\n",
    " - `boot:` iterates over the bootstrap paths and puts it into a dataframe\n",
    " - `plot_realizations:` plots $n$ (default 50) randomly chosen paths to get and idea of the simulated returns paths. This is visualized when calling the simulation method.\n",
    " - `plot_summary_stats:` visualizes the histograms from all the statistics when calling the bootstrap method (the default setting is not showing since this is a bit too much output)\n",
    " - `investment_horizons:` wraps around the functions from above and iterates over the simulation methods\n",
    " - `permutation_plot:` visualizes the permuted sample statistic(s) of interest. Note this will show the same histogram as those in `plot_summary_stats`\n",
    " - `append_stats:` appends the calculated statistic together in a pandas dataframe for each:\n",
    "      - strategy\n",
    "      - simulation method\n",
    "      - investment horizon\n",
    "\n",
    " - `performance_matrix:` calculates the summary statistics of the permuated histograms\n",
    " - `describe_dist:` shows the summary statisitcs (calculated in `performance_matrix:`)  in a multi index dataframe\n",
    "\n",
    "\n",
    "### Class\n",
    "\n",
    " - `Simulations:` a wrapper around the function `investment_horizon`. It contains 5 methods to perform the simulation:\n",
    "      - sample from a normal distribution\n",
    "      - iid boostrap\n",
    "      - mbb\n",
    "      - cbb \n",
    "      - sb\n",
    "      \n",
    "      \n",
    "All information regarding the bootstrap is stored into a dictionary **store_output_dic**. This dictionary consist out of several sub dictionaries. You can think of it as a tree structure: \n",
    "\n",
    " -  At the highest level you have the **investment portfolios**\n",
    " \n",
    "       - each portfolio contains the chosen **simulation methods**\n",
    "       \n",
    "            - each portfolio on his turn contains a dictionary **Total sample length and Investment horizon** which contains the calculated sample statistic(s)  \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:52:31.116296Z",
     "start_time": "2019-04-14T23:52:31.098341Z"
    }
   },
   "outputs": [],
   "source": [
    "perf_stats_dic.keys()  # statisitcs to choose form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:52:31.130255Z",
     "start_time": "2019-04-14T23:52:31.117290Z"
    }
   },
   "outputs": [],
   "source": [
    "sum_stats = ['Geometric Return (%)', 'Maximum Drawdown (%)']\n",
    "store_output_dic = {}  # store all the output in a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Market Factor (Mkt-RF) <a id='3.1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:52:55.461187Z",
     "start_time": "2019-04-14T23:52:31.132251Z"
    }
   },
   "outputs": [],
   "source": [
    "market_simulation = Simulation(data=df_factors.loc[:, \"Mkt-RF\"],\n",
    "                               n_paths=N_PATHS,\n",
    "                               blocksize=BLOCKSIZE,\n",
    "                               stats=sum_stats,\n",
    "                               perf_functions=perf_stats_dic,\n",
    "                               investment_horizon=IH,\n",
    "                               frequency=FREQUENCY,\n",
    "                               plotting=PLOT_INTERMEDIATE,\n",
    "                               store_output=store_output_dic)\n",
    "\n",
    "market_simulation.normal()\n",
    "market_simulation.iid_bootstrap()\n",
    "#market_simulation.mbb_bootstrap()\n",
    "market_simulation.cbb_bootstrap()\n",
    "market_simulation.sb_bootstrap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:52:55.478132Z",
     "start_time": "2019-04-14T23:52:55.467162Z"
    }
   },
   "outputs": [],
   "source": [
    "market_simulation.store_output.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value (HML) <a id='3.2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:53:19.660452Z",
     "start_time": "2019-04-14T23:52:55.482122Z"
    }
   },
   "outputs": [],
   "source": [
    "value_simulation = Simulation(data=df_factors.loc[:, \"HML\"],\n",
    "                              n_paths=N_PATHS,\n",
    "                              blocksize=BLOCKSIZE,\n",
    "                              stats=sum_stats,\n",
    "                              perf_functions=perf_stats_dic,\n",
    "                              investment_horizon=IH,\n",
    "                              frequency=FREQUENCY,\n",
    "                              plotting=PLOT_INTERMEDIATE,\n",
    "                              store_output=store_output_dic)\n",
    "\n",
    "value_simulation.normal()\n",
    "value_simulation.iid_bootstrap()\n",
    "# value_simulation.mbb_bootstrap()\n",
    "value_simulation.cbb_bootstrap()\n",
    "value_simulation.sb_bootstrap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:53:19.680399Z",
     "start_time": "2019-04-14T23:53:19.662447Z"
    }
   },
   "outputs": [],
   "source": [
    "value_simulation.store_output.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum (WML) <a id='3.3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:53:45.198158Z",
     "start_time": "2019-04-14T23:53:19.683392Z"
    }
   },
   "outputs": [],
   "source": [
    "momentum_simulation = Simulation(data=df_factors.loc[:, \"WML\"],\n",
    "                                 n_paths=N_PATHS,\n",
    "                                 blocksize=BLOCKSIZE,\n",
    "                                 stats=sum_stats,\n",
    "                                 perf_functions=perf_stats_dic,\n",
    "                                 investment_horizon=IH,\n",
    "                                 frequency=FREQUENCY,\n",
    "                                 plotting=PLOT_INTERMEDIATE,\n",
    "                                 store_output=store_output_dic)\n",
    "\n",
    "momentum_simulation.normal()\n",
    "momentum_simulation.iid_bootstrap()\n",
    "# momentum_simulation.mbb_bootstrap()\n",
    "momentum_simulation.cbb_bootstrap()\n",
    "momentum_simulation.sb_bootstrap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:53:45.208121Z",
     "start_time": "2019-04-14T23:53:45.201139Z"
    }
   },
   "outputs": [],
   "source": [
    "momentum_simulation.store_output.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50-50 value-momentum <a id='3.4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:54:09.192969Z",
     "start_time": "2019-04-14T23:53:45.210115Z"
    }
   },
   "outputs": [],
   "source": [
    "val_mom_simulation = Simulation(data=df_factors.loc[:, \"HML-WML\"],\n",
    "                                n_paths=N_PATHS,\n",
    "                                blocksize=BLOCKSIZE,\n",
    "                                stats=sum_stats,\n",
    "                                perf_functions=perf_stats_dic,\n",
    "                                investment_horizon=IH,\n",
    "                                frequency=FREQUENCY,\n",
    "                                plotting=PLOT_INTERMEDIATE,\n",
    "                                store_output=store_output_dic)\n",
    "\n",
    "val_mom_simulation.normal()\n",
    "val_mom_simulation.iid_bootstrap()\n",
    "# val_mom_simulation.mbb_bootstrap()\n",
    "val_mom_simulation.cbb_bootstrap()\n",
    "val_mom_simulation.sb_bootstrap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:54:09.206932Z",
     "start_time": "2019-04-14T23:54:09.194964Z"
    }
   },
   "outputs": [],
   "source": [
    "val_mom_simulation.store_output.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## summarize results <a id='3.5'></a>\n",
    "\n",
    "I will start by making some figures in order to summarizes the information efficiently. Next, I do the same thing numerically.\n",
    "\n",
    "\n",
    "**Statistics:**\n",
    "  \n",
    "  - geometric return \n",
    "  - maximum drawdown\n",
    "  - number of positive returns (based on the geometric return)\n",
    "\n",
    "**Portfolios:**\n",
    "\n",
    " - Mkt-RF\n",
    " - HML\n",
    " - WML\n",
    " - HML-WML\n",
    " \n",
    "**Simulation methods**\n",
    "\n",
    "- normal\n",
    "- iid bootstrapping\n",
    "- cbb\n",
    "- sb\n",
    "\n",
    "**Investment horizons (year)**\n",
    "- 1, 3, 5, 10, 20, 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Visually\n",
    "\n",
    "#### 1. Geometric return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:54:10.784711Z",
     "start_time": "2019-04-14T23:54:09.208926Z"
    }
   },
   "outputs": [],
   "source": [
    "permutation_plot(dic=val_mom_simulation.store_output,\n",
    "                 investment_h=30,\n",
    "                 statistic='Geometric Return (%)',\n",
    "                 period=\"1927-2019\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Maximum Drawdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:54:12.871130Z",
     "start_time": "2019-04-14T23:54:10.786706Z"
    }
   },
   "outputs": [],
   "source": [
    "permutation_plot(dic=val_mom_simulation.store_output,\n",
    "                 investment_h=30,\n",
    "                 statistic='Maximum Drawdown (%)',\n",
    "                 period=\"1927-2019\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:54:13.237152Z",
     "start_time": "2019-04-14T23:54:12.873125Z"
    }
   },
   "outputs": [],
   "source": [
    "statistic = \"Geometric Return (%)\"\n",
    "df_stat = append_stats(stat=statistic, dic=val_mom_simulation.store_output)\n",
    "df_stat.tail()\n",
    "\n",
    "above_zero = df_stat.groupby(\n",
    "    [\"Strategy\", \"Investment Horizon (year)\", \"Simulation technique\"],\n",
    "    sort=False)[statistic].apply(lambda g: (g < 0).sum()).reset_index(\n",
    "        name='count')\n",
    "above_zero.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Positive return for different investment horizon (based on geometric return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:54:16.054617Z",
     "start_time": "2019-04-14T23:54:13.239150Z"
    }
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(2, 3, figsize=(20, 14))\n",
    "axes = axes.ravel()\n",
    "i = 0\n",
    "for h, h_df in above_zero.groupby(\"Investment Horizon (year)\"):\n",
    "\n",
    "    sns.barplot(y=\"Simulation technique\",\n",
    "                x=\"count\",\n",
    "                hue=\"Strategy\",\n",
    "                data=h_df,\n",
    "                ax=axes[i])\n",
    "    axes[i].get_legend().set_visible(False)  # remove legend\n",
    "    axes[i].set_ylabel(\"Simulation method\")\n",
    "    axes[i].set_title(f'Investment Horizon: {h} year',\n",
    "                      fontsize=18,\n",
    "                      fontweight=\"bold\",\n",
    "                      y=1.02)\n",
    "    axes[i].grid(True, which='major', linestyle='-.', color='0.8')\n",
    "    i += 1\n",
    "plt.suptitle(f\"Number of paths with a negative\\\n",
    " final value conditioned on investment horizon and factor\\n ({N_PATHS :,}\\\n",
    " bootstrap samples, blocksize = {BLOCKSIZE} days)\",\n",
    "             y=1.15,\n",
    "             fontsize=25,\n",
    "             fontweight=\"bold\",\n",
    "             style=\"italic\")\n",
    "\n",
    "# remove unnecessary labels\n",
    "axes[1].set_ylabel(\"\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "axes[4].set_ylabel(\"\")\n",
    "axes[5].set_ylabel(\"\")\n",
    "axes[0].set_xlabel(\"\")\n",
    "axes[1].set_xlabel(\"\")\n",
    "axes[2].set_xlabel(\"\")\n",
    "# set legend\n",
    "plt.tight_layout()\n",
    "axes[1].legend(bbox_to_anchor=(1.175, 1.275), ncol=4)\n",
    "\n",
    "# save\n",
    "plt.savefig('Positive_Return', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:54:17.111788Z",
     "start_time": "2019-04-14T23:54:16.059604Z"
    }
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(2, 2, figsize=(20, 10))\n",
    "axes = axes.ravel()\n",
    "i = 0\n",
    "x = np.array([1, 2, 3, 4, 5, 6])\n",
    "for s1, d1 in above_zero.groupby([\"Strategy\"], sort=False):\n",
    "    for s2, d2 in d1.groupby(\"Simulation technique\", sort=False):\n",
    "        axes[i].plot(x, d2[\"count\"], label=s2, linestyle='--', marker='o')\n",
    "\n",
    "        axes[i].set_title(f'Investment Strategy: {s1}',\n",
    "                          fontsize=18,\n",
    "                          fontweight=\"bold\",\n",
    "                          y=1.02)\n",
    "        axes[i].grid(True, which='major', linestyle='-.', color='0.8')\n",
    "\n",
    "    axes[i].set_xticklabels([\"1\", \"1\", \"3\", \"5\", \"10\", \"20\", \"30\"])\n",
    "    axes[i].set_xlabel(\"Investment horizon (year)\")\n",
    "    i += 1\n",
    "axes[0].set_ylabel(\"count\")\n",
    "axes[0].set_xlabel(\"\")\n",
    "axes[2].set_ylabel(\"count\")\n",
    "axes[1].set_xlabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "axes[1].legend(bbox_to_anchor=(.3, 1.35), ncol=4)\n",
    "plt.suptitle(\n",
    "    f\"Number of paths with a negative final value condtioned on investment\\\n",
    "horizon and factor\\n ({N_PATHS :,} bootstrap samples, blocksize = {BLOCKSIZE} days)\",\n",
    "    y=1.2,\n",
    "    fontsize=25,\n",
    "    fontweight=\"bold\",\n",
    "    style=\"italic\");\n",
    "# save\n",
    "#plt.savefig('Positive_Return_line', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Numeric Summary Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mkt-RF\n",
    "   #### 1. Geometric return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:54:17.767037Z",
     "start_time": "2019-04-14T23:54:17.112786Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "round(\n",
    "    performance_matrix(dic=val_mom_simulation.store_output,\n",
    "                       strategy=\"Mkt-RF\",\n",
    "                       statistic=\"Geometric Return (%)\",\n",
    "                       sim_meth=[\"Normal\", \"IB\", \"CBB\", \"SB\"],\n",
    "                       nr_neg=True), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Maximum Drawdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:54:18.181926Z",
     "start_time": "2019-04-14T23:54:17.769030Z"
    }
   },
   "outputs": [],
   "source": [
    "round(\n",
    "    performance_matrix(dic=val_mom_simulation.store_output,\n",
    "                       strategy=\"Mkt-RF\",\n",
    "                       statistic=\"Maximum Drawdown (%)\",\n",
    "                       nr_neg=False), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HML\n",
    "   #### 1. Geometric return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:54:18.535979Z",
     "start_time": "2019-04-14T23:54:18.184919Z"
    }
   },
   "outputs": [],
   "source": [
    "round(\n",
    "    performance_matrix(dic=val_mom_simulation.store_output,\n",
    "                       strategy=\"HML\",\n",
    "                       statistic=\"Geometric Return (%)\",\n",
    "                       nr_neg=True), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Maximum Drawdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:54:18.916960Z",
     "start_time": "2019-04-14T23:54:18.536977Z"
    }
   },
   "outputs": [],
   "source": [
    "round(\n",
    "    performance_matrix(dic=val_mom_simulation.store_output,\n",
    "                       strategy=\"HML\",\n",
    "                       statistic=\"Maximum Drawdown (%)\",\n",
    "                       nr_neg=False), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WML\n",
    "   #### 1. Geometric return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:54:19.301931Z",
     "start_time": "2019-04-14T23:54:18.917957Z"
    }
   },
   "outputs": [],
   "source": [
    "round(\n",
    "    performance_matrix(dic=val_mom_simulation.store_output,\n",
    "                       strategy=\"WML\",\n",
    "                       statistic=\"Geometric Return (%)\",\n",
    "                       nr_neg=True), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Maximum Drawdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:54:19.717831Z",
     "start_time": "2019-04-14T23:54:19.307915Z"
    }
   },
   "outputs": [],
   "source": [
    "round(\n",
    "    performance_matrix(dic=val_mom_simulation.store_output,\n",
    "                       strategy=\"WML\",\n",
    "                       statistic=\"Maximum Drawdown (%)\",\n",
    "                       nr_neg=False), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HML-WML\n",
    "   #### 1. Geometric return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:54:20.189556Z",
     "start_time": "2019-04-14T23:54:19.720810Z"
    }
   },
   "outputs": [],
   "source": [
    "round(\n",
    "    performance_matrix(dic=val_mom_simulation.store_output,\n",
    "                       strategy=\"HML-WML\",\n",
    "                       statistic=\"Geometric Return (%)\",\n",
    "                       nr_neg=True), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Maximum Drawdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:54:20.734100Z",
     "start_time": "2019-04-14T23:54:20.191552Z"
    }
   },
   "outputs": [],
   "source": [
    "round(\n",
    "    performance_matrix(dic=val_mom_simulation.store_output,\n",
    "                       strategy=\"HML-WML\",\n",
    "                       statistic=\"Maximum Drawdown (%)\",\n",
    "                       nr_neg=False), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a clear distinction between the distributions depending on the simulation assumptions:\n",
    "\n",
    " - the normal and i.i.d. bootstrap are similar\n",
    " - and the circular block bootstrap and stationary bootstrap are more alike\n",
    " \n",
    "IMO the circular and stationary bootstrap givs a far more  realistic picture about expectations, since they (try to) preserve the non-normality and serial correlations. One limitation of the circular block bootstrap is the fixed block size. Different block sizes emphasize different periods or lengths of autocorrelation (memory). At the extremes you can take a block size so small (i.i.d. bootstrap) that no serial correlation is captured, and at the other end you could take a block size so large that you end up sampling the original series. However, in my initial experimentation there are many reasonable block sizes that generate realistic price paths, that overall capture the behavior of the original series. I guess this is also why results of the circular block bootstrap are very close to those obtained from the stationary bootstrap. \n",
    "\n",
    "\n",
    "**Geometric Return** \n",
    "\n",
    "Perhaps most interesting are the **frequencies of negative returns (# neg. R) for different factors under a investment horizon ranging from 1 year to 30 years**.\n",
    "When evaluating portfolio performance, professional investors often put much weight on three or five years of past returns. The message from the results above is that chance is a big player in outcomes for these horizons. In the simulations, the expected premiums are more or less equal to their sample averages. Negative simulated premiums are then just chance results of high return volatility. By definition the variance will shrink as you increase the investor horizon, the question remains how fast?\n",
    "\n",
    "As an example, **For a five-year investment horizon**, the estimated probabilities that a **value strategy** delivers a negative premium on a purely chance basis are substantial, ranging between 20-26% depending on the simulation assumptions. The estimated frequencies of negative premiums are bigger at shorter investment horiozns, even though we know that, by construction, the expected premiums are positive and large. For longer investment horizons the negative premium becomes less likely but still substantial for 10-year periods and they are also nontrivial for 20-year periods. \n",
    "\n",
    "**Maximum Drawdown**\n",
    "\n",
    " \n",
    "The main message when looking at drawdowns is two-fold. First, it shows that value of and momentum strategies like the market also suffers large drawdowns. Moreover, if we falsely view returns as being well approximated by independent normal distributions, you would severely underestimate the magnitude of the worst likely drawdowns. Second,  if we account for the serial correlation structures, the observed large drawdowns are not that surprising at all. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Chapter4'></a>\n",
    "# Chapter: 4 Simulation Analysis Part 2\n",
    "\n",
    "## Recent poor performance of the Momentum and Value factor \n",
    "\n",
    "The figure below shows that value and momentum factor did not deliver much return\n",
    "in the **last 15 years (01-01-2004 to 28-02-2019)**. Are these factors broken or just experiencing a period of bad luck? To answer this question, I examine the role of bad luck in explaining the recent poor performance of factors. I start with measuring the performance of each factor over the last 15 years\n",
    "ending February 2019 and use simulations to assess the likelihood of observing this level of\n",
    "performance, given how the factors performed before these recent years. I compare factor performance from **January 2004** through **February 2019** to performance from:\n",
    " - **July 1963 through December 2003**. \n",
    " - **January 1927 through December 2003**\n",
    " \n",
    "Similar as in Chapter 3, I then simulate returns under different assumptions to assess which features of factor returns, if any, can account for their poor recent performance. Some Papers **only include data starting from 1963** because they argue that the period before is not representative anymore due to higher volatility. Therefore, I will analyze both periods separately and look whether there are notable differences. **The number of simulations is set at 10,000. and the blocksize and mean blocksize are both 126 days**. Finally, the bootstrap samples where I calculate the statistic of interest on are based on an investment horizon of 15 years.\n",
    "\n",
    "\n",
    "### Helper functions\n",
    " - `p_values:`calculates the p-values\n",
    " - `p_table:`appends the p-values for different statistics , investment horizons, \n",
    "    strategies and simulation methods in a table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance over the last recent 15 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:54:22.472451Z",
     "start_time": "2019-04-14T23:54:20.777983Z"
    }
   },
   "outputs": [],
   "source": [
    "start = datetime.date(year=2004, month=1, day=1)\n",
    "df_15 = df_factors.loc[start:]\n",
    "Return_DD(df=df_15, dic_labels=dic_fac, time_frequency=FREQUENCY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:54:22.542264Z",
     "start_time": "2019-04-14T23:54:22.474445Z"
    }
   },
   "outputs": [],
   "source": [
    "print(round(df_15.describe(), 3))\n",
    "obs_15 = round(perf_stat(df_15, stats=\"all\", dic_perf=perf_stats_dic), 3)\n",
    "obs_15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance  before the last 15 years starting from 1963"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:54:24.491051Z",
     "start_time": "2019-04-14T23:54:22.544258Z"
    }
   },
   "outputs": [],
   "source": [
    "start, end = datetime.date(year=1963, month=1, day=1), datetime.date(year=2004,\n",
    "                                                                     month=1,\n",
    "                                                                     day=1)\n",
    "df_before_15_63 = df_factors.loc[start:end]\n",
    "Return_DD(df=df_before_15_63, dic_labels=dic_fac, time_frequency=FREQUENCY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance  before the last 15 years starting from 1927 (all data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:54:26.501674Z",
     "start_time": "2019-04-14T23:54:24.493046Z"
    }
   },
   "outputs": [],
   "source": [
    "start, end = datetime.date(year=1927, month=1, day=1), datetime.date(year=2004,\n",
    "                                                                     month=1,\n",
    "                                                                     day=1)\n",
    "df_before_15_27 = df_factors.loc[start:end]\n",
    "Return_DD(df=df_before_15_27, dic_labels=dic_fac,time_frequency=FREQUENCY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:54:26.513644Z",
     "start_time": "2019-04-14T23:54:26.505662Z"
    }
   },
   "outputs": [],
   "source": [
    "perf_stats_dic.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:54:26.527606Z",
     "start_time": "2019-04-14T23:54:26.515637Z"
    }
   },
   "outputs": [],
   "source": [
    "sum_stats = ['Geometric Return (%)', 'Maximum Drawdown (%)', 'Geometric SR']\n",
    "investment_horizon = [15]  # in years\n",
    "# store all the output in a dictionary\n",
    "store_output_dic_63, store_output_dic_27 = {}, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation Period: 01-01-1963 till 31-12-2003 <a id='4.1'></a>\n",
    "###  Market <a id='4.1.1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:54:41.020839Z",
     "start_time": "2019-04-14T23:54:26.528603Z"
    }
   },
   "outputs": [],
   "source": [
    "market_simulation_15_63 = Simulation(data=df_before_15_63.loc[:, \"Mkt-RF\"],\n",
    "                                     n_paths=N_PATHS,\n",
    "                                     blocksize=BLOCKSIZE,\n",
    "                                     stats=sum_stats,\n",
    "                                     perf_functions=perf_stats_dic,\n",
    "                                     investment_horizon=investment_horizon,\n",
    "                                     frequency=FREQUENCY,\n",
    "                                     plotting=PLOT_INTERMEDIATE,\n",
    "                                     store_output=store_output_dic_63)\n",
    "\n",
    "market_simulation_15_63.normal()\n",
    "market_simulation_15_63.iid_bootstrap()\n",
    "market_simulation_15_63.cbb_bootstrap()\n",
    "market_simulation_15_63.sb_bootstrap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:54:41.033804Z",
     "start_time": "2019-04-14T23:54:41.027821Z"
    }
   },
   "outputs": [],
   "source": [
    "market_simulation_15_63.store_output.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value (HML) <a id='4.1.2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:54:54.320268Z",
     "start_time": "2019-04-14T23:54:41.037794Z"
    }
   },
   "outputs": [],
   "source": [
    "value_simulation_15_63 = Simulation(data=df_before_15_63.loc[:, \"HML\"],\n",
    "                                     n_paths=N_PATHS,\n",
    "                                     blocksize=BLOCKSIZE,\n",
    "                                     stats=sum_stats,\n",
    "                                     perf_functions=perf_stats_dic,\n",
    "                                     investment_horizon=investment_horizon,\n",
    "                                     frequency=FREQUENCY,\n",
    "                                     plotting=PLOT_INTERMEDIATE,\n",
    "                                     store_output=store_output_dic_63)\n",
    "\n",
    "value_simulation_15_63.normal()\n",
    "value_simulation_15_63.iid_bootstrap()\n",
    "value_simulation_15_63.cbb_bootstrap()\n",
    "value_simulation_15_63.sb_bootstrap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:54:54.329243Z",
     "start_time": "2019-04-14T23:54:54.323260Z"
    }
   },
   "outputs": [],
   "source": [
    "value_simulation_15_63.store_output.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Momentum (WML) <a id='4.1.3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:55:07.072160Z",
     "start_time": "2019-04-14T23:54:54.331238Z"
    }
   },
   "outputs": [],
   "source": [
    "momentum_simulation_15_63 = Simulation(data=df_before_15_63.loc[:, \"WML\"],\n",
    "                                       n_paths=N_PATHS,\n",
    "                                       blocksize=BLOCKSIZE,\n",
    "                                       stats=sum_stats,\n",
    "                                       perf_functions=perf_stats_dic,\n",
    "                                       investment_horizon=investment_horizon,\n",
    "                                       frequency=FREQUENCY,\n",
    "                                       plotting=PLOT_INTERMEDIATE,\n",
    "                                       store_output=store_output_dic_63)\n",
    "\n",
    "momentum_simulation_15_63.normal()\n",
    "momentum_simulation_15_63.iid_bootstrap()\n",
    "momentum_simulation_15_63.cbb_bootstrap()\n",
    "momentum_simulation_15_63.sb_bootstrap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:55:07.080139Z",
     "start_time": "2019-04-14T23:55:07.074156Z"
    }
   },
   "outputs": [],
   "source": [
    "momentum_simulation_15_63.store_output.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value-Momentum (HML-WML) <a id='4.1.3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:55:19.687418Z",
     "start_time": "2019-04-14T23:55:07.082134Z"
    }
   },
   "outputs": [],
   "source": [
    "val_mom_simulation_15_63 = Simulation(data=df_before_15_63.loc[:, \"HML-WML\"],\n",
    "                                      n_paths=N_PATHS,\n",
    "                                      blocksize=BLOCKSIZE,\n",
    "                                      stats=sum_stats,\n",
    "                                      perf_functions=perf_stats_dic,\n",
    "                                      investment_horizon=investment_horizon,\n",
    "                                      frequency=FREQUENCY,\n",
    "                                      plotting=PLOT_INTERMEDIATE,\n",
    "                                      store_output=store_output_dic_63)\n",
    "\n",
    "val_mom_simulation_15_63.normal()\n",
    "val_mom_simulation_15_63.iid_bootstrap()\n",
    "val_mom_simulation_15_63.cbb_bootstrap()\n",
    "val_mom_simulation_15_63.sb_bootstrap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:55:19.696395Z",
     "start_time": "2019-04-14T23:55:19.689413Z"
    }
   },
   "outputs": [],
   "source": [
    "val_mom_simulation_15_63.store_output.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation Period: 01-01-1927 till 31-12-2003 <a id='4.2'></a>\n",
    "\n",
    "### Market (Mkt-RF) <a id='4.2.1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:55:41.325543Z",
     "start_time": "2019-04-14T23:55:19.698389Z"
    }
   },
   "outputs": [],
   "source": [
    "market_simulation_15_27 = Simulation(data=df_before_15_27.loc[:, \"Mkt-RF\"],\n",
    "                                     n_paths=N_PATHS,\n",
    "                                     blocksize=BLOCKSIZE,\n",
    "                                     stats=sum_stats,\n",
    "                                     perf_functions=perf_stats_dic,\n",
    "                                     investment_horizon=investment_horizon,\n",
    "                                     frequency=FREQUENCY,\n",
    "                                     plotting=PLOT_INTERMEDIATE,\n",
    "                                     store_output=store_output_dic_27)\n",
    "\n",
    "market_simulation_15_27.normal()\n",
    "market_simulation_15_27.iid_bootstrap()\n",
    "market_simulation_15_27.cbb_bootstrap()\n",
    "market_simulation_15_27.sb_bootstrap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:55:41.333522Z",
     "start_time": "2019-04-14T23:55:41.326542Z"
    }
   },
   "outputs": [],
   "source": [
    "market_simulation_15_27.store_output.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Value (HML) <a id='4.2.2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:56:00.362627Z",
     "start_time": "2019-04-14T23:55:41.336514Z"
    }
   },
   "outputs": [],
   "source": [
    "value_simulation_15_27 = Simulation(data=df_before_15_27.loc[:, \"HML\"],\n",
    "                                     n_paths=N_PATHS,\n",
    "                                     blocksize=BLOCKSIZE,\n",
    "                                     stats=sum_stats,\n",
    "                                     perf_functions=perf_stats_dic,\n",
    "                                     investment_horizon=investment_horizon,\n",
    "                                     frequency=FREQUENCY,\n",
    "                                     plotting=PLOT_INTERMEDIATE,\n",
    "                                     store_output=store_output_dic_27)\n",
    "\n",
    "value_simulation_15_27.normal()\n",
    "value_simulation_15_27.iid_bootstrap()\n",
    "value_simulation_15_27.cbb_bootstrap()\n",
    "value_simulation_15_27.sb_bootstrap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:56:00.377596Z",
     "start_time": "2019-04-14T23:56:00.365617Z"
    }
   },
   "outputs": [],
   "source": [
    "value_simulation_15_27.store_output.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum (WML) <a id='4.2.3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:56:18.892067Z",
     "start_time": "2019-04-14T23:56:00.380578Z"
    }
   },
   "outputs": [],
   "source": [
    "momentum_simulation_15_27 = Simulation(data=df_before_15_27.loc[:, \"WML\"],\n",
    "                                     n_paths=N_PATHS,\n",
    "                                     blocksize=BLOCKSIZE,\n",
    "                                     stats=sum_stats,\n",
    "                                     perf_functions=perf_stats_dic,\n",
    "                                     investment_horizon=investment_horizon,\n",
    "                                     frequency=FREQUENCY,  \n",
    "                                     plotting=PLOT_INTERMEDIATE,\n",
    "                                     store_output=store_output_dic_27)\n",
    "\n",
    "momentum_simulation_15_27.normal()\n",
    "momentum_simulation_15_27.iid_bootstrap()\n",
    "momentum_simulation_15_27.cbb_bootstrap()\n",
    "momentum_simulation_15_27.sb_bootstrap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:56:18.913008Z",
     "start_time": "2019-04-14T23:56:18.896056Z"
    }
   },
   "outputs": [],
   "source": [
    "momentum_simulation_15_27.store_output.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value-Momentum (HML-WML) <a id='4.2.4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:56:37.685797Z",
     "start_time": "2019-04-14T23:56:18.916001Z"
    }
   },
   "outputs": [],
   "source": [
    "val_mom_simulation_15_27 = Simulation(data=df_before_15_27.loc[:, \"HML-WML\"],\n",
    "                                     n_paths=N_PATHS,\n",
    "                                     blocksize=BLOCKSIZE,\n",
    "                                     stats=sum_stats,\n",
    "                                     perf_functions=perf_stats_dic,\n",
    "                                     investment_horizon=investment_horizon,\n",
    "                                      frequency=FREQUENCY,\n",
    "                                     plotting=PLOT_INTERMEDIATE,\n",
    "                                     store_output=store_output_dic_27)\n",
    "\n",
    "val_mom_simulation_15_27.normal()\n",
    "val_mom_simulation_15_27.iid_bootstrap()\n",
    "val_mom_simulation_15_27.cbb_bootstrap()\n",
    "val_mom_simulation_15_27.sb_bootstrap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:56:37.703750Z",
     "start_time": "2019-04-14T23:56:37.687793Z"
    }
   },
   "outputs": [],
   "source": [
    "val_mom_simulation_15_27.store_output.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize results <a id='4.3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation Period: 01-01-1963 till 31-12-2003 <a id='4.3.1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:56:37.721703Z",
     "start_time": "2019-04-14T23:56:37.705744Z"
    }
   },
   "outputs": [],
   "source": [
    "sum_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:56:37.773564Z",
     "start_time": "2019-04-14T23:56:37.723697Z"
    }
   },
   "outputs": [],
   "source": [
    "statistic = 'Geometric Return (%)'  # could choose any of the statistic above\n",
    "dic_output_63 = val_mom_simulation_15_63.store_output\n",
    "df_stat_63 = append_stats(stat=statistic, dic=dic_output_63)\n",
    "df_stat_63.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometric Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:56:37.791518Z",
     "start_time": "2019-04-14T23:56:37.774561Z"
    }
   },
   "outputs": [],
   "source": [
    "# observed performance over the last 15 years\n",
    "perf_stat(df=df_15, stats=[\"Geometric Return (%)\"], dic_perf=perf_stats_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:56:38.724021Z",
     "start_time": "2019-04-14T23:56:37.794510Z"
    }
   },
   "outputs": [],
   "source": [
    "obs_T = perf_stat(df=df_15, stats=[\"Geometric Return (%)\"],dic_perf=perf_stats_dic).values.tolist()[0]\n",
    "permutation_plot(dic=val_mom_simulation_15_63.store_output,\n",
    "                 investment_h=15,\n",
    "                 statistic='Geometric Return (%)',\n",
    "                 period=\"1963-2004\",\n",
    "                 obs_stats=obs_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Drawdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:56:38.742973Z",
     "start_time": "2019-04-14T23:56:38.726015Z"
    }
   },
   "outputs": [],
   "source": [
    "# observed maximum dd over the last 15 years\n",
    "perf_stat(df=df_15, stats=[\"Maximum Drawdown (%)\"],dic_perf=perf_stats_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:56:39.693428Z",
     "start_time": "2019-04-14T23:56:38.744965Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "obs_T = perf_stat(df=df_15, stats=[\"Maximum Drawdown (%)\"],dic_perf=perf_stats_dic).values.tolist()[0]\n",
    "permutation_plot(dic=val_mom_simulation_15_63.store_output,\n",
    "                 investment_h=15,\n",
    "                 statistic='Maximum Drawdown (%)',\n",
    "                 period=\"1963-2004\",\n",
    "                 obs_stats=obs_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric Summary Table: 01-01-1963 till 31-12-2003\n",
    "\n",
    "The table below shows the number of returns paths with a more extreme statistic (more to the left except for drawdown this is right). The number of simulations was set at 10,000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:56:39.943908Z",
     "start_time": "2019-04-14T23:56:39.694425Z"
    }
   },
   "outputs": [],
   "source": [
    "p_table(stat=sum_stats,\n",
    "        dic=val_mom_simulation_15_63.store_output,\n",
    "        observed=df_15, \n",
    "        perf_stats_dic=perf_stats_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations regarding the geometric return**\n",
    "\n",
    "Below I describe my observations for the **annualized geometric return**, similar conclusions hold for other performance statistics (except maybe for maximum drawdown)\n",
    "\n",
    "The permuted distributions excludes the 15-year period ending 28 February 2019 from the analysis in creating\n",
    "the 10,000 simulated histories of factor returns. If an investor uses these data to calibrate\n",
    "expectations about performance over the next 15 years, **how probable would the actual\n",
    "observed geometric annual returns seem?**\n",
    "\n",
    "As in chapter 3, I use a blocksize of 126 for the circular block bootstrap and the average blocksize for the stationary bootstrap is also set at 126 days. The p-value associated with the observed  returns are very low except for the market portfolio. Relative to the powerful returns from before 2004, the recent performance of the value, momentum and the value-momentum strategies over the past 15 years is unexpectedly low. There certainly are differences between the different simulation methods, though the main conclusion does not change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation Period: 01-01-1927 till 31-12-2003 <a id='4.3.2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometric return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:56:40.971130Z",
     "start_time": "2019-04-14T23:56:39.945872Z"
    }
   },
   "outputs": [],
   "source": [
    "obs_T = perf_stat(df=df_15, stats=[\"Geometric Return (%)\"],dic_perf=perf_stats_dic).values.tolist()[0]\n",
    "permutation_plot(dic=val_mom_simulation_15_27.store_output,\n",
    "                 investment_h=15,\n",
    "                 statistic='Geometric Return (%)',\n",
    "                 period=\"1927-2004\",\n",
    "                 obs_stats=obs_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Drawdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:56:42.005364Z",
     "start_time": "2019-04-14T23:56:40.972127Z"
    }
   },
   "outputs": [],
   "source": [
    "obs_T = perf_stat(df=df_15, stats=[\"Maximum Drawdown (%)\"],dic_perf=perf_stats_dic).values.tolist()[0]\n",
    "permutation_plot(dic=val_mom_simulation_15_27.store_output,\n",
    "                 investment_h=15,\n",
    "                 statistic=\"Maximum Drawdown (%)\",\n",
    "                 period=\"1927-2004\",\n",
    "                 obs_stats=obs_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric Summary Table: 01-01-1927 till 31-12-2003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below shows the number of returns paths with a more extreme statistic (more to to left except for drawdown this is right). The number of simulations was set at 10,000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:56:42.294591Z",
     "start_time": "2019-04-14T23:56:42.008358Z"
    }
   },
   "outputs": [],
   "source": [
    "p_table(stat=sum_stats,\n",
    "        dic=val_mom_simulation_15_27.store_output,\n",
    "        observed=df_15,\n",
    "        perf_stats_dic=perf_stats_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations regarding the geometric return**\n",
    "\n",
    "I repeat the excercise from above, but when constructing the permuted distribution I use all data starting from 01-01-1927 till 31-12-2003. All other parameters are kept the same as before. Looking at the p-values in the table above, the probability of observing the performance for various statisitcs over the last 15 years increases,  but stays rather low. This is certainly the case for the value-momentum strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Chapter5'></a>\n",
    "# Chapter 5: Conclusion and discussion\n",
    "\n",
    "\n",
    "**Investment horizon matters**\n",
    " - The probabilities that premiums are negative on a purely chance basis are substantial for the 1-year, 3-year, 5-year and 10-year periods. Chance is big player in outcomes for these horizons.\n",
    " - Large differences noticeable conditioned on the simulation assumptions and investment strategy: still the main message does not change.\n",
    " - Deciding on the (mean) block size is not trivial, but there are many reasonable (mean) block sizes that generate realistic price paths, that overall capture the behavior of the original series.\n",
    " - Any study such as this suffers from survivor bias: the US stock market was one of the most successful investment choices an investor could have made when looking back. As a result, we are probably underestimating/overestimating depending on the statistic.\n",
    " \n",
    "**Recent past performance of value, momentum and a 50-50 value-momentum strategies are unexpectedly low**\n",
    "\n",
    "It is no secret that value and momentum strategies have recently fallen far short of investor expectations. Is this a case of the factors being broken or have they just been unlucky over the last 15 years? The answer is probably a combination of both. Recent factor performance has been uncharacteristically bad given pre-2004 performance. Value (HML), momentum (WML) and 50-50 value-momentum (HML-WML) pre-2004 returns were likely inflated due to data mining and selection bias, and their post 2004 returns were likely depressed by crowding  as the factors gained widespread adoption. In contrary, the performance of the market portfolio (Mkt-RF) over the last 15 years is in line with historical expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Chapter6'></a>\n",
    "# Chapter 6: Limitations\n",
    "\n",
    "**Assumptions being made**\n",
    "\n",
    "\n",
    "- all data over the whole sample is representative: contrary to other people I do think that data from 1927 is still useful. I believe  events like the crash in 1930 can happen again (actually the magnitude of 2007 was close to 1930).\n",
    "  \n",
    "- the the sampled blocks are iid\n",
    " \n",
    "- stationarity\n",
    " \n",
    "- there are clear volatility regimes: these have clear impact on our bootstrap results. What are the implications?\n",
    " \n",
    "- drawdown is a question mark for me\n",
    " \n",
    "- survivorship bias: the US stock market was one of the best investment choices an investor could have made. Therefore results in Chapter 3 are probably underestimated (in the negative sense) and are not generalizable to other countries. Furthermore, even in the US, very few investment firms outperform (on various statistics) the portfolios considered in this study (even before slippage and transactions costs).\n",
    " \n",
    "- I do not account for slippage and transaction cost. Slippage is the difference between where the computer signaled the entry and exit for a trade and where actual clients with actual money, entered and exited the market using the computers signals. By definition you would never have had the same price as the historical reported price since you would have moved the price most likely against you. There are however clever ways to minimize this cost. I do not think this is a problem for Chapter 4 as long as you compare apples with apples.\n",
    "\n",
    "- I assume an investor reinvest his money (no withdrawals), this may not be realistic in reality\n",
    "\n",
    "- other important assumptions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Chapter7'></a>\n",
    "# Chapter 7: Questions\n",
    "\n",
    "\n",
    "1. Some general comments on the analysis: \n",
    "\n",
    "      - thoughts, suggestions, thing I should pay attention to...\n",
    "\n",
    "\n",
    "2. Normally when bootstrapping the distribution of the statistic(s) of interest you use all data you have. However, since I am interested in the distribution of the statistic for a specific investment horizon, I condition on the bootstrap length. Is this problematic and what are the consequences? \n",
    "\n",
    "3. Regarding maximum drawdown, can I bootstrap such a statistic? I read some arguments about being careful when bootstrapping extrema statistics  **(https://stats.stackexchange.com/questions/119748/using-bootstrap-to-obtain-sampling-distribution-of-1st-percentile), [[6]](http://www.stat.rutgers.edu/home/mxie/stat586/handout/Bootstrap1.pdf)**. For WML and WML-HML, you get a bivariate distribution. I assume this has to do with the volatility regimes?\n",
    "\n",
    "4. These volatility regimes have a clear impact on our bootstrap results. What are the implications?\n",
    "5. What are the assumptions being made regarding the block bootstrap (cbb and sb): \n",
    "  \n",
    "    - for example we assume these blocks are iid \n",
    "    - how reasonable are these approaches considered for this problem and under what circumstances are they not appropriate? \n",
    "\n",
    "\n",
    "6. Regarding the stationary bootstrap: when is this bootstrap method more appropriate compared to the circular block bootstrap?\n",
    "7. What important assumptions did I made, not mentioned\n",
    "8. I also looked at the moving block bootstrap, but got very similar result to the circular and stationary block bootstrap. I would like to have a better understanding what assumptions I have made and which bootstraps methods are most appropriate in what circumstances. Regarding the block size, several rules have been suggested when choosing the block the block length. So far I have found several reasonable block sizes that seem to generate realistic price paths, that overall capture the behavior of the original series. Furthermore, the application of stationary bootstrap is less sensitive to the choice of block length compared to the circular block bootstrap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Chapter8'></a>\n",
    "# Chapter 8: References\n",
    "\n",
    "## Papers\n",
    "- **[1]  [Arnott, R. D., Harvey, C. R., Kalesnik, V., & Linnainmaa, J. T. (2019). Alice’s Adventures in Factorland: Three Blunders  That Plague Factor Investing. Available at SSRN 3331680.](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3331680)**\n",
    "- **[2]  [Fama, E. F., & French, K. R. (2018). Long-horizon returns. The Review of Asset Pricing Studies, 8(2), 232-252.](https://academic.oup.com/raps/article-abstract/8/2/232/4810768)**\n",
    "- **[3]  [Fama, E. F., & French, K. R. (2012). Size, value, and momentum in international stock returns. Journal of financial economics, 105(3), 457-472.](https://www.sciencedirect.com/science/article/pii/S0304405X12000931)**\n",
    "- **[4]  [Daniel, K., & Moskowitz, T. J. (2016). Momentum crashes. Journal of Financial Economics, 122(2), 221-247.](https://www.sciencedirect.com/science/article/pii/S0304405X16301490)**\n",
    "- **[5]  [Radovanov, B., & Marcikić, A. (2014). A comparison of four different block bootstrap methods. Croatian Operational Research Review, 5(2), 189-202.](https://scholar.google.be/scholar?hl=nl&as_sdt=0%2C5&q=Radovanov%2C+B.%2C+%26+Marciki%C4%87%2C+A.+%282014%29.+A+comparison+of+four+different+block+bootstrap+methods.+Croatian+Operational+Research+Review%2C+5%282%29%2C+189-202&btnG=)**\n",
    "\n",
    "- **[6] [Singh, K., & Xie, M. (2008). Bootstrap: a statistical method. Unpublished manuscript, Rutgers University, USA. Retrieved from http://www. stat. rutgers. edu/home/mxie/RCPapers/bootstrap. pdf.](http://www.stat.rutgers.edu/home/mxie/stat586/handout/Bootstrap1.pdf)**\n",
    "- **[7] [Efron, B. (1992). Bootstrap methods: another look at the jackknife. In Breakthroughs in statistics (pp. 569-593). Springer, New York, NY.](https://scholar.google.be/scholar?hl=nl&as_sdt=0%2C5&q=Efron%2C+B.+%281979%29.+Bootstrap+methods%3A+Another+look+at+jackknife.+Ann.+Stat.+7%2C+1-26&btnG=)**\n",
    "\n",
    "## Blogs, websites, presentations ...\n",
    "\n",
    "- **[8]  http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html**\n",
    "- **[9]  http://keplerianfinance.com/2013/06/the-relevance-of-history/**\n",
    "- **[10]  https://www.youtube.com/watch?v=27x632vOjXk&t=3349s**\n",
    "- **[11]  https://www.youtube.com/watch?v=BtiqH_7d2es&t=28s**\n",
    "- **[12] http://www.blackarbs.com/blog/synthetic-data-generation-part-1-block-bootstrapping**\n",
    "- **[13] https://arch.readthedocs.io/en/latest/bootstrap/timeseries-bootstraps.html**\n",
    "- **[14] https://pandas-datareader.readthedocs.io/en/latest/remote_data.html**\n",
    "- **[15] http://www.ievbras.ru/ecostat/Kiril/R/Biblio_N/R_Eng/Chernick2011.pdf?fbclid=IwAR1f8UAc6s7FAGykclIB0usn0U9RGibNf6sB5ug2z9DEx9DoCK7Q-8nMVLk**\n",
    "- **[16] http://www.quantdevel.com/public/CSP2015/Talk/BootstrappingTimeSeriesData.pdf?fbclid=IwAR3FiKvHjfrg9zGZkm8aa07vMT15PkpxAYFLqvR50uBdGIjOgM0AY0nsd2U**"
   ]
  }
 ],
 "metadata": {
  "gist": {
   "data": {
    "description": "Simulation_RawR.ipynb",
    "public": true
   },
   "id": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": "0",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "409.6px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
